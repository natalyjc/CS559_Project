{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Final Project </center>\n",
    "## <center> Group 3 </center>\n",
    "\n",
    "## Elia Rezaeian\n",
    "\n",
    "_________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "### SECTION3 : Train Model-Stacking Method\n",
    "_________________________________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Section1&2(Clusters).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>Predicted_Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.658210</td>\n",
       "      <td>-1.400730</td>\n",
       "      <td>-0.381088</td>\n",
       "      <td>0.452337</td>\n",
       "      <td>0.858035</td>\n",
       "      <td>-0.012784</td>\n",
       "      <td>-0.381166</td>\n",
       "      <td>0.117564</td>\n",
       "      <td>0.221323</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.428676</td>\n",
       "      <td>0.461506</td>\n",
       "      <td>-1.030601</td>\n",
       "      <td>-0.234852</td>\n",
       "      <td>-0.029953</td>\n",
       "      <td>-0.033227</td>\n",
       "      <td>-0.566027</td>\n",
       "      <td>0.076135</td>\n",
       "      <td>0.440510</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.775424</td>\n",
       "      <td>1.442919</td>\n",
       "      <td>-0.711948</td>\n",
       "      <td>0.144689</td>\n",
       "      <td>-0.152528</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>0.258936</td>\n",
       "      <td>-0.555779</td>\n",
       "      <td>-0.286176</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.403655</td>\n",
       "      <td>4.045684</td>\n",
       "      <td>0.326334</td>\n",
       "      <td>-1.356795</td>\n",
       "      <td>-0.638412</td>\n",
       "      <td>-0.146502</td>\n",
       "      <td>-0.058012</td>\n",
       "      <td>0.546713</td>\n",
       "      <td>-0.348422</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.865429</td>\n",
       "      <td>0.283041</td>\n",
       "      <td>0.526507</td>\n",
       "      <td>-0.195726</td>\n",
       "      <td>-0.437119</td>\n",
       "      <td>-0.979682</td>\n",
       "      <td>-0.145639</td>\n",
       "      <td>-0.450059</td>\n",
       "      <td>0.824183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0  2.658210 -1.400730 -0.381088  0.452337  0.858035 -0.012784 -0.381166   \n",
       "1 -1.428676  0.461506 -1.030601 -0.234852 -0.029953 -0.033227 -0.566027   \n",
       "2 -2.775424  1.442919 -0.711948  0.144689 -0.152528  0.164080  0.258936   \n",
       "3  2.403655  4.045684  0.326334 -1.356795 -0.638412 -0.146502 -0.058012   \n",
       "4  0.865429  0.283041  0.526507 -0.195726 -0.437119 -0.979682 -0.145639   \n",
       "\n",
       "        PC8       PC9  Cluster  Bankrupt?  Predicted_Cluster  \n",
       "0  0.117564  0.221323        1          0                  1  \n",
       "1  0.076135  0.440510        5          0                  5  \n",
       "2 -0.555779 -0.286176        5          0                  5  \n",
       "3  0.546713 -0.348422        0          0                  0  \n",
       "4 -0.450059  0.824183        0          0                  0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the data into X and Y and then I select companies in clusters 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5802    0\n",
       "5803    0\n",
       "5804    0\n",
       "5805    0\n",
       "5806    0\n",
       "Name: Bankrupt?, Length: 5807, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:-3]\n",
    "y = df.iloc[:,-2]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1231, 545)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_0 = X[df.iloc[:,-1] == 0] \n",
    "X_1 = X[df.iloc[:,-1] == 1]\n",
    "\n",
    "y_0 = y[df.iloc[:,-1] == 0]\n",
    "y_1 = y[df.iloc[:,-1] == 1]\n",
    "len(y_0), len(y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I wrote a function for models Tuning that we can apply on differtent data and scoring using Stratified Kfold and Gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that have all base models with tuning their hyperparameters using grid search\n",
    "\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['Logistic'] = LogisticRegression(solver='liblinear',random_state=42)\n",
    "    models['SVM'] = SVC(random_state=42)\n",
    "    models['KNN'] = KNeighborsClassifier()\n",
    "    models['RandomForest'] = RandomForestClassifier(random_state=42)\n",
    "    # models['xgboost'] = xgb.XGBClassifier(random_state=42)\n",
    "    models['NeuralNetwork'] = MLPClassifier(random_state=42)\n",
    "    models['DecisionTree'] = DecisionTreeClassifier(random_state=42)\n",
    "    models['AdaBoost'] = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42),random_state=42)\n",
    "    models['GradientBoosting'] = GradientBoostingClassifier(random_state=42)\n",
    "    models['NaiveBayes'] = GaussianNB()\n",
    "    models['Bagging'] = BaggingClassifier(random_state=42)\n",
    "    \n",
    "    return models\n",
    "\n",
    "def get_params():\n",
    "    \n",
    "    params = dict()\n",
    "    params['Logistic'] =  { 'penalty': ['l1', 'l2', 'elasticnet', 'none'],'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'class_weight': ['balanced', None], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],'max_iter': [100, 200, 300, 400, 500]}\n",
    "    \n",
    "    params['SVM'] = {'C': [ 0.01, 0.1, 1 ], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],'degree': [2, 3, 4, 5],\n",
    "    'gamma': ['scale', 'auto'], 'class_weight': ['balanced']}\n",
    "    \n",
    "    params['KNN'] = {'n_neighbors': [3, 5, 7, 9, 11], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],'p': [1, 2]}\n",
    "    \n",
    "    params['RandomForest'] = param_grid = {'n_estimators': [100, 200, 300, 400], 'max_depth': [None, 10, 20, 30, 40],   \n",
    "    'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4],'bootstrap': [True, False],'class_weight': ['balanced', 'balanced_subsample'],  \n",
    "    'max_features': ['auto', 'sqrt', 'log2']}\n",
    "    \n",
    "    params['xgboost'] = {'max_depth': [3, 4, 5, 6],'min_child_weight': [1, 2, 3],'gamma': [0, 0.1, 0.2, 0.3],'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],'learning_rate': [0.01, 0.1, 0.2, 0.3],'n_estimators': [100, 200, 300]}\n",
    "    \n",
    "    params['NeuralNetwork'] = {'hidden_layer_sizes': [(100,), (50, 50), (100, 100), (50, 50, 50)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],'solver': ['lbfgs', 'sgd', 'adam'],'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],'max_iter': [200, 300, 400]}\n",
    "    \n",
    "    params['DecisionTree'] = {'max_depth': [None, 10, 20, 30, 50], 'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],'criterion': ['gini', 'entropy']}\n",
    "    \n",
    "    params['AdaBoost'] = {'n_estimators': [50, 100, 200, 300],'learning_rate': [0.01, 0.1, 1, 10],\n",
    "    'base_estimator__max_depth': [1, 2, 3]}\n",
    "    \n",
    "    params['GradientBoosting'] = {'n_estimators': [100, 200, 300, 400],'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'max_depth': [3, 5, 8],'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4],'max_features': ['sqrt', 'log2', None]}\n",
    "    \n",
    "    params['NaiveBayes'] = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
    "    \n",
    "    params['Bagging'] = {'n_estimators': [10, 50, 100, 200],'max_samples': [0.5, 0.7, 1.0], 'max_features': [0.5, 0.7, 1.0],  \n",
    "    'bootstrap': [True, False],'bootstrap_features': [True, False]}\n",
    "    \n",
    "    return params\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def get_best_model(X, y, models, params, score):\n",
    "    best_model = dict()\n",
    "    for key in models.keys():\n",
    "        model = models[key]\n",
    "        param = params[key]\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle = True, random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param, n_jobs=-1, cv=cv, scoring = score)\n",
    "        grid_result = grid_search.fit(X, y)\n",
    "        best_model[key] = grid_result.best_estimator_\n",
    "        # Print the best model and best score for each type of model\n",
    "        print(f\"Best Model for {key}: {grid_result.best_estimator_}\")\n",
    "        print(f\"Best Parameters {key}: {grid_result.best_params_}\")\n",
    "        print(f\"Best Score for {key}: {grid_result.best_score_:.4f}\\n\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter tuning for cluster 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m models_0 \u001b[38;5;241m=\u001b[39m get_models()\n\u001b[0;32m      4\u001b[0m params_0 \u001b[38;5;241m=\u001b[39m get_params()\n\u001b[1;32m----> 5\u001b[0m best_model_0 \u001b[38;5;241m=\u001b[39m get_best_model(X_0, y_0, models_0, params_0, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 67\u001b[0m, in \u001b[0;36mget_best_model\u001b[1;34m(X, y, models, params, score)\u001b[0m\n\u001b[0;32m     65\u001b[0m cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     66\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39mcv, scoring \u001b[38;5;241m=\u001b[39m score)\n\u001b[1;32m---> 67\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     68\u001b[0m best_model[key] \u001b[38;5;241m=\u001b[39m grid_result\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Print the best model and best score for each type of model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\mkear\\anaconda3\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#apply on cluster 0\n",
    "\n",
    "models_0 = get_models()\n",
    "params_0 = get_params()\n",
    "best_model_0 = get_best_model(X_0, y_0, models_0, params_0, 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I selected 3 best model from the previous cell and apply a meta learner using logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 0 \n",
    "# Best models for cluster 0 which we can use in our meta model\n",
    "LR_0 = best_model_0['Logistic']\n",
    "SVM_0 = best_model_0['SVM']\n",
    "NN_0 = best_model_0['NeuralNetwork']\n",
    "\n",
    "model_0_1 = LR_0.fit(X_0,y_0)\n",
    "model_0_2 = SVM_0.fit(X_0,y_0)\n",
    "model_0_3 = NN_0.fit(X_0,y_0)\n",
    "\n",
    "LR_pred_0 = model_0_1.predict(X_0)\n",
    "SVM_pred_0 = model_0_2.predict(X_0)\n",
    "NN_pred_0 = model_0_3.predict(X_0)\n",
    "\n",
    "print(\"Logistic Regression confustion matrix:\\n\",confusion_matrix(y_0,LR_pred_0 ))\n",
    "print(\"SVM confustion matrix:\\n\",confusion_matrix(y_0,SVM_pred_0 ))\n",
    "print(\"NN confustion matrix:\\n\",confusion_matrix(y_0,NN_pred_0 ))\n",
    "\n",
    "X_Stack_0 = pd.DataFrame()\n",
    "X_Stack_0['NN'] = NN_pred_0\n",
    "X_Stack_0['SVM'] = SVM_pred_0\n",
    "X_Stack_0['LR'] = LR_pred_0\n",
    "\n",
    "meta_learner = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "meta_learner.fit(X_Stack_0, y_0)\n",
    "\n",
    "meta_learner_pred = meta_learner.predict(X_Stack_0)\n",
    "meta_learner_classification_report = classification_report(y_0, meta_learner_pred)\n",
    "print(\"Meta learner Classification report:\\n\",meta_learner_classification_report)\n",
    "meta_learner_cm = confusion_matrix(y_0, meta_learner_pred)\n",
    "print(\"Meta learner Confusion Matrix:\\n\",meta_learner_cm)\n",
    "accuracy_score_meta = accuracy_score(y_0, meta_learner_pred)\n",
    "print(\"Meta learner Accuracy:\\n\", accuracy_score_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the meta learner\n",
    "from joblib import dump\n",
    "dump(meta_learner, 'meta_learner1.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The TT/(TT+TF) for logistic regression is = > 75%\n",
    "* The TT/(TT+TF) for SVM is = > 62%\n",
    "* The TT/(TT+TF) Neural Network is = > 100% \n",
    "\n",
    "when I use meta learner in the for ensemble stacking, I got 100% accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I tune the hyperparameters on Cluster 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply on cluster 1\n",
    "\n",
    "models_1 = get_models()\n",
    "params_1 = get_params()\n",
    "best_model_1 = get_best_model(X_1, y_1, models_1, params_1, 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I selected 3 best model from the previous cell and apply a meta learner using logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best models for cluster 0 which we can use in our meta model\n",
    "LR_1 = best_model_0['Logistic']\n",
    "SVM_1 = best_model_0['SVM']\n",
    "NN_0 = best_model_0['NeuralNetwork']\n",
    "\n",
    "model_1_1 = LR_0.fit(X_1,y_1)\n",
    "model_1_2 = SVM_0.fit(X_1,y_1)\n",
    "model_1_3 = NN_0.fit(X_1,y_1)\n",
    "\n",
    "LR_pred_1 = model_1_1.predict(X_1)\n",
    "SVM_pred_1 = model_1_2.predict(X_1)\n",
    "NN_pred_1 = model_1_3.predict(X_1)\n",
    "\n",
    "print(\"Logistic Regression confustion matrix:\\n\",confusion_matrix(y_1,LR_pred_1 ))\n",
    "print(\"SVM confustion matrix:\\n\",confusion_matrix(y_1,SVM_pred_1 ))\n",
    "print(\"NN confustion matrix:\\n\",confusion_matrix(y_1,NN_pred_1 ))\n",
    "\n",
    "X_Stack_1 = pd.DataFrame()\n",
    "X_Stack_1['NN'] = NN_pred_1\n",
    "X_Stack_1['SVM'] = SVM_pred_1\n",
    "X_Stack_1['LR'] = LR_pred_1\n",
    "\n",
    "meta_learner1 = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "meta_learner1.fit(X_Stack_1, y_1)\n",
    "\n",
    "meta_learner_pred1 = meta_learner1.predict(X_Stack_1)\n",
    "meta_learner_classification_report1 = classification_report(y_1, meta_learner_pred1)\n",
    "print(\"Meta learner Classification report:\\n\",meta_learner_classification_report1)\n",
    "meta_learner_cm1 = confusion_matrix(y_1, meta_learner_pred1)\n",
    "print(\"Meta learner Confusion Matrix:\\n\",meta_learner_cm1)\n",
    "accuracy_score_meta1 = accuracy_score(y_1, meta_learner_pred1)\n",
    "print(\"Meta learner Accuracy:\\n\", accuracy_score_meta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The TT/(TT+TF) for logistic regression is = > 80%\n",
    "* The TT/(TT+TF) for SVM is = > 60%\n",
    "* The TT/(TT+TF) Neural Network is = > 100% \n",
    "\n",
    "when I use meta learner in the for ensemble stacking, I got 100% accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________________________________________________\n",
    "### SECTION4 : K_Fold Cross Validation on the Whole Dataset\n",
    "_________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Section1&2(Clusters).csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:9]\n",
    "y = df.iloc[:,10]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here I first run all models without tuning on the whole dataset and got results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_whole_data1 = {}\n",
    "RANDOM_STATE = 42\n",
    "# Initialize the classifier\n",
    "models1 = {\n",
    "    'LogisticRegression': LogisticRegression(solver='liblinear', random_state=42),\n",
    "    'SVC': SVC(random_state=42),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'XGBClassifier': xgb.XGBClassifier(random_state=42),\n",
    "    'MLPClassifier': MLPClassifier(random_state=42),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'BaggingClassifier': BaggingClassifier(random_state=42)\n",
    "}\n",
    "# Use stratified kfold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True ,random_state=RANDOM_STATE)\n",
    "\n",
    "for model_name, model in models1.items():\n",
    "    recall_scores = cross_val_score(model, X, y, cv=skf, scoring='recall')\n",
    "    mean_scores = recall_scores.mean()\n",
    "    # find cm\n",
    "    y_pred = cross_val_predict(model, X, y, cv=skf)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    result_whole_data1[model_name] = (cm, mean_scores, accuracy)\n",
    "\n",
    "# print(\"Results for whole data:\")\n",
    "# print(result_whole_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here shows that while the accuracy is high but they cannot predict the Instanses with label 1 well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"LogisticRegression confustion matrix:\\n\", result_whole_data1['LogisticRegression'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['LogisticRegression'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['LogisticRegression'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"SVC confustion matrix:\\n\", result_whole_data1['SVC'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['SVC'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['SVC'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"KNeighborsClassifier confustion matrix:\\n\", result_whole_data1['KNeighborsClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['KNeighborsClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['KNeighborsClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"RandomForestClassifier confustion matrix:\\n\", result_whole_data1['RandomForestClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['RandomForestClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['RandomForestClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"XGBClassifier confustion matrix:\\n\", result_whole_data1['XGBClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['XGBClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['XGBClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"MLPClassifier confustion matrix:\\n\", result_whole_data1['MLPClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['MLPClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['MLPClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"DecisionTreeClassifier confustion matrix:\\n\", result_whole_data1['DecisionTreeClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['DecisionTreeClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['DecisionTreeClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"AdaBoostClassifier confustion matrix:\\n\", result_whole_data1['AdaBoostClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['AdaBoostClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['AdaBoostClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"GradientBoostingClassifier confustion matrix:\\n\", result_whole_data1['GradientBoostingClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['GradientBoostingClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['GradientBoostingClassifier'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"GaussianNB confustion matrix:\\n\", result_whole_data1['GaussianNB'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['GaussianNB'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['GaussianNB'][2])\n",
    "print(\"-------------------------------------\")\n",
    "print(\"BaggingClassifier confustion matrix:\\n\", result_whole_data1['BaggingClassifier'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data1['BaggingClassifier'][1])\n",
    "print(\"Accuracy = \", result_whole_data1['BaggingClassifier'][2])\n",
    "print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we tried to tune the hyperparameter of all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that have all base models with tuning their hyperparameters using grid search\n",
    "\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['Logistic'] = LogisticRegression(solver='liblinear',random_state=42)\n",
    "    models['SVM'] = SVC(random_state=42)\n",
    "    models['KNN'] = KNeighborsClassifier()\n",
    "    models['RandomForest'] = RandomForestClassifier(random_state=42)\n",
    "    models['xgboost'] = xgb.XGBClassifier(random_state=42)\n",
    "    models['NeuralNetwork'] = MLPClassifier(random_state=42)\n",
    "    models['AdaBoost'] = AdaBoostClassifier(random_state=42)\n",
    "    models['gradiantboost'] = GradientBoostingClassifier(random_state=42)\n",
    "    models['Bagging'] = BaggingClassifier(random_state=42)\n",
    "\n",
    "    \n",
    "    return models\n",
    "\n",
    "def get_params():\n",
    "    \n",
    "    params = dict()\n",
    "    params['Logistic'] =  { 'penalty': ['l1', 'l2', 'elasticnet', 'none'],'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'class_weight': ['balanced', None], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],'max_iter': [100, 200, 300, 400, 500]}\n",
    "    \n",
    "    params['SVM'] = {'C': [ 0.01, 0.1, 1 ], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],'degree': [2, 3, 4, 5],\n",
    "    'gamma': ['scale', 'auto'], 'class_weight': ['balanced']}\n",
    "    \n",
    "    params['KNN'] = {'n_neighbors': [3, 5, 7, 9, 11], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],'p': [1, 2]}\n",
    "    \n",
    "    params['RandomForest'] = param_grid = {'n_estimators': [100, 200, 300, 400], 'max_depth': [None, 10, 20, 30, 40],   \n",
    "    'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4],'bootstrap': [True],'class_weight': ['balanced'],  \n",
    "    'max_features': ['auto', 'sqrt', 'log2']}\n",
    "    \n",
    "    params['NeuralNetwork'] = {'hidden_layer_sizes': [(100,), (50, 50), (100, 100), (50, 50, 50)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],'solver': ['lbfgs', 'sgd', 'adam'],'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],'max_iter': [200, 300, 400]}\n",
    "    \n",
    "    params['xgboost'] = {'max_depth': [3, 4, 5, 6],'min_child_weight': [1, 2, 3],'gamma': [0, 0.1, 0.2, 0.3],'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],'learning_rate': [0.01, 0.1, 0.2, 0.3],'n_estimators': [100, 200, 300]}\n",
    "    \n",
    "    params['AdaBoost'] = {'n_estimators': [50, 100, 200, 300],'learning_rate': [0.01, 0.1, 1, 10],'algorithm': ['SAMME', 'SAMME.R']} \n",
    "  \n",
    "    params['gradiantboost'] = {'n_estimators': [50, 100, 200, 300],'learning_rate': [0.01, 0.1, 1, 10],'max_depth': [3, 4, 5, 6]}\n",
    "    \n",
    "    params['Bagging'] = {'n_estimators': [10, 50, 100, 200],'max_samples': [0.5, 1.0],'max_features': [0.5, 1.0]}   \n",
    "        \n",
    "    return params\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def get_best_model(X, y, models, params, score):\n",
    "    best_model = dict()\n",
    "    for key in models.keys():\n",
    "        model = models[key]\n",
    "        param = params[key]\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle= True, random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param, n_jobs=-1, cv=cv, scoring = score)\n",
    "        grid_result = grid_search.fit(X, y)\n",
    "        best_model[key] = grid_result.best_estimator_\n",
    "        # Print the best model and best score for each type of model\n",
    "        print(f\"Best Model for {key}: {grid_result.best_estimator_}\")\n",
    "        print(f\"Best Parameters {key}: {grid_result.best_params_}\")\n",
    "        print(f\"Best Score for {key}: {grid_result.best_score_:.4f}\\n\")\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is imbalanced we need to tune our model to predict class 1 well. so for tuning the models I use f1 score to have the models suitable for imbalanced data. f1 score is the average of predision and recall and make balance between these two so that we can predict both class 0 and 1 good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()\n",
    "params = get_params()\n",
    "best_model= get_best_model(X, y, models, params, 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I used the tuned models for the k fold cross validation and got much better results here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use kfold on whole data with random forest, XGboost and Bagging\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "result_whole_data = {}\n",
    "RANDOM_STATE = 42\n",
    "# Initialize the classifier\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced', min_samples_leaf=4,min_samples_split=10, n_estimators=200, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=RANDOM_STATE, colsample_bytree= 0.9, gamma =0.2, learning_rate= 0.3, max_depth = 4, min_child_weight= 3, n_estimators= 300, subsample= 0.8),\n",
    "    'Bagging': BaggingClassifier(random_state=RANDOM_STATE),\n",
    "    'Logistic Regression': LogisticRegression(C=1, penalty='l1', random_state=42, solver='liblinear'),\n",
    "    \"SVM\" : SVC(C=0.01, class_weight='balanced', degree=5, kernel='poly', random_state=42),\n",
    "    'KNN': KNeighborsClassifier(leaf_size=10, p=1, weights='distance'),\n",
    "    'NN':MLPClassifier(activation = 'relu', alpha= 0.0001, hidden_layer_sizes= (100,), learning_rate= 'constant', max_iter= 400, solver= 'adam', random_state = 42),\n",
    "    'AdaBoost' : AdaBoostClassifier(algorithm='SAMME.R', learning_rate=1, n_estimators=50, random_state=42),\n",
    "    'GradientBoost': GradientBoostingClassifier(learning_rate=1, max_depth=3, n_estimators=50, random_state=42),\n",
    "    'Bagging': BaggingClassifier(max_features=1, max_samples=0.5, n_estimators=200, random_state=42)\n",
    "}\n",
    "# Use stratified kfold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    recall_scores = cross_val_score(model, X, y, cv=skf, scoring='recall')\n",
    "    mean_scores = recall_scores.mean()\n",
    "    # find cm\n",
    "    y_pred = cross_val_predict(model, X, y, cv=skf)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    result_whole_data[model_name] = (cm, mean_scores, acc)\n",
    "\n",
    "# print(\"Results for whole data:\")\n",
    "# print(result_whole_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest confustion matrix:\\n\", result_whole_data['Random Forest'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['Random Forest'][1])\n",
    "print('accuracy:', result_whole_data['Random Forest'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"XGBoost confustion matrix:\\n\", result_whole_data['XGBoost'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['XGBoost'][1])\n",
    "print('accuracy:', result_whole_data['XGBoost'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"Bagging confustion matrix:\\n\", result_whole_data['Bagging'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['Bagging'][1])\n",
    "print('accuracy:', result_whole_data['Bagging'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"Logistic Regression confustion matrix:\\n\", result_whole_data['Logistic Regression'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['Logistic Regression'][1])\n",
    "print('accuracy:', result_whole_data['Logistic Regression'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"SVM confustion matrix:\\n\", result_whole_data['SVM'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['SVM'][1])\n",
    "print('accuracy:', result_whole_data['SVM'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"KNN confustion matrix:\\n\", result_whole_data['KNN'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['KNN'][1])\n",
    "print('accuracy:', result_whole_data['KNN'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"NN confustion matrix:\\n\", result_whole_data['NN'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['NN'][1])\n",
    "print('accuracy:', result_whole_data['NN'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"AdaBoost confustion matrix:\\n\", result_whole_data['AdaBoost'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['AdaBoost'][1])\n",
    "print('accuracy:', result_whole_data['AdaBoost'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"GradientBoost confustion matrix:\\n\", result_whole_data['GradientBoost'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['GradientBoost'][1])\n",
    "print('accuracy:', result_whole_data['GradientBoost'][2])\n",
    "print(\"------------------------------------\")\n",
    "print(\"Bagging confustion matrix:\\n\", result_whole_data['Bagging'][0])\n",
    "print(\"TT/(TT+TF) = \", result_whole_data['Bagging'][1])\n",
    "print('accuracy:', result_whole_data['Bagging'][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as you can see, SVM has the best results. \n",
    "\n",
    "*SVM confustion matrix:\\\n",
    "\n",
    "[[5191      418]\\\n",
    "[  49       149]]\n",
    "\n",
    "* TT/(TT+TF) =  0.7521794871794871\n",
    "* accuracy: 0.9195798174616842"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
